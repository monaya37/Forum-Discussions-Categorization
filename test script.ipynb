{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(\"WednesdayTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleID</th>\n",
       "      <th>Discussion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Possibly he is a misspelling of the hero of Ay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Like anal fisting teens?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Look into these sites:\\nhttp://www.ukparks.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NOVEMBER 03, 2004 (IDG NEWS SERVICE) - An anon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Of course they take a SMALL commissioned rate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SampleID                                         Discussion\n",
       "0         1  Possibly he is a misspelling of the hero of Ay...\n",
       "1         2                           Like anal fisting teens?\n",
       "2         3  Look into these sites:\\nhttp://www.ukparks.com...\n",
       "3         4  NOVEMBER 03, 2004 (IDG NEWS SERVICE) - An anon...\n",
       "4         5  Of course they take a SMALL commissioned rate ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in Test Dataset:\n",
      "SampleID      0\n",
      "Discussion    0\n",
      "dtype: int64\n",
      "\n",
      "Missing Values in Test Dataset After Filling:\n",
      "SampleID      0\n",
      "Discussion    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:01<00:00, 30.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "#####################################   PREPROCESSING    #####################################\n",
    "\n",
    "########### 1. FILLING NULLS\n",
    "\n",
    "print(\"\\nMissing Values in Test Dataset:\")\n",
    "print(test_csv.isnull().sum())\n",
    "\n",
    "test_csv = test_csv.fillna('')\n",
    "print(\"\\nMissing Values in Test Dataset After Filling:\")\n",
    "print(test_csv.isnull().sum())\n",
    "\n",
    "\n",
    "########### 4. REMOVE NOISE FROM TEXT\n",
    "\n",
    "def normalize_repeated_chars(word):\n",
    "    repeat_regexp = re.compile(r'(.)\\1{2,}')  # Match characters repeated 3 or more times\n",
    "    word = repeat_regexp.sub(r'\\1', word)  # Replace them with a single occurrence\n",
    "    return word\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # (handles NaN or other types)\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Dictionary of contractions\n",
    "    contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                        \"could've\": \"could have\",\n",
    "                        \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "                        \"hadn't\": \"had not\",\n",
    "                        \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "                        \"he's\": \"he is\",\n",
    "                        \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                        \"I'd\": \"I would\",\n",
    "                        \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
    "                        \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                        \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
    "                        \"i've\": \"i have\",\n",
    "                        \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
    "                        \"it'll've\": \"it will have\",\n",
    "                        \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "                        \"might've\": \"might have\",\n",
    "                        \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                        \"mustn't\": \"must not\",\n",
    "                        \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                        \"o'clock\": \"of the clock\",\n",
    "                        \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                        \"sha'n't\": \"shall not\",\n",
    "                        \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                        \"she'll\": \"she will\",\n",
    "                        \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                        \"shouldn't\": \"should not\",\n",
    "                        \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\",\n",
    "                        \"that'd\": \"that would\",\n",
    "                        \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                        \"there'd've\": \"there would have\",\n",
    "                        \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
    "                        \"they'd've\": \"they would have\",\n",
    "                        \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "                        \"they've\": \"they have\",\n",
    "                        \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
    "                        \"we'll\": \"we will\",\n",
    "                        \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                        \"what'll\": \"what will\",\n",
    "                        \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "                        \"what've\": \"what have\",\n",
    "                        \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                        \"where've\": \"where have\",\n",
    "                        \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                        \"why's\": \"why is\",\n",
    "                        \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                        \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                        \"y'all\": \"you all\",\n",
    "                        \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\",\n",
    "                        \"y'all've\": \"you all have\",\n",
    "                        \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "                        \"you'll've\": \"you will have\",\n",
    "                        \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "    def _get_contractions(contraction_dict):\n",
    "        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "        return contraction_dict, contraction_re\n",
    "\n",
    "    def replace_contractions(text):\n",
    "        contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "        def replace(match):\n",
    "            return contractions[match.group(0)]\n",
    "\n",
    "        return contractions_re.sub(replace, text)\n",
    "\n",
    "    text = replace_contractions(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Remove user mentions (e.g., @username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Replace weird things like \\n\n",
    "    text = re.sub(r'\\\\n', ' ', text)\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Normalize repeated characters (e.g., \"noooooooooooo\" -> \"no\")\n",
    "    words = text.split()\n",
    "    words = [normalize_repeated_chars(word) for word in words]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    emoticon_pattern = re.compile(r\"[:;=8xX]-?[)D(P/\\\\\\|]\")\n",
    "    text = emoticon_pattern.sub(r'', text)\n",
    "\n",
    "    # Tokenize and lemmatize using SpaCy\n",
    "    doc = nlp(text)  # Apply lemmatization after correction\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc if token.text not in stopwords.words('english')])\n",
    "\n",
    "    return lemmatized_text.lower()\n",
    "\n",
    "test_csv['Discussion'] = test_csv['Discussion'].progress_apply(preprocess_text)\n",
    "\n",
    "\n",
    "########### 5. REMOVE STOPWORDS\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "custom_stopwords = set(stop) - {'not', 'no', 'but'}  # Remove from stopwords 'not', 'no', 'but'\n",
    "custom_stopwords = custom_stopwords.union({'use', 'one', 'go', 'think', 'make'})\n",
    "test_csv['Discussion'] = test_csv['Discussion'].astype(str).apply(lambda z: \" \".join(z for z in z.lower().split() if z not in custom_stopwords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 6. TOKENIZE & PAD\n",
    "from keras.preprocessing.sequence import pad_sequences  # Correct import\n",
    "\n",
    "max_words = 10000\n",
    "max_sequence_length = 480\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "path = r'models pickled\\tokenizer.pkl'\n",
    "# Load the pickled tokenizer\n",
    "with open(path, \"rb\") as file:\n",
    "    tokenizer = (pickle.load(file))\n",
    "\n",
    "#tokenizer.fit_on_texts(test_csv) ############### heya dy elly et3amalaha pickle\n",
    "\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_csv['Discussion'])\n",
    "test_pad = pad_sequences(test_seq, maxlen=max_sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 343ms/step\n"
     ]
    }
   ],
   "source": [
    "########### 8. LOAD MODEL (SIMPLE RNN)\n",
    "from keras.models import load_model\n",
    "\n",
    "simple_path = r'models pickled\\rnn_model1_0.70.h5'\n",
    "# Load the model\n",
    "rnn_model1 = load_model(simple_path)\n",
    "# Predict categories for the test data\n",
    "rnn_model1_predictions = rnn_model1.predict(test_pad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0127262 , 0.01467933, 0.02964983, 0.16647701, 0.77646756],\n",
       "       [0.07250869, 0.12786977, 0.2061121 , 0.1753914 , 0.418118  ],\n",
       "       [0.00570137, 0.00270727, 0.01575111, 0.16671193, 0.8091282 ],\n",
       "       ...,\n",
       "       [0.15125938, 0.18022051, 0.25550494, 0.22327152, 0.18974374],\n",
       "       [0.4217559 , 0.02322108, 0.03769602, 0.40575993, 0.11156708],\n",
       "       [0.11939061, 0.10822439, 0.53914946, 0.18192254, 0.05131301]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model1_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SampleID  Category\n",
      "0         1         2\n",
      "1         2         2\n",
      "2         3         4\n",
      "3         4         4\n",
      "4         5         3\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = rnn_model1_predictions.argmax(axis=1)\n",
    "\n",
    "sample_ids = np.arange(1, len(predicted_classes) + 1)\n",
    "    \n",
    "output_df = pd.DataFrame({\n",
    "    \"SampleID\": sample_ids,\n",
    "    \"Category\": predicted_classes\n",
    "})\n",
    "    \n",
    "output_df.to_csv('rnn1_test.csv', index=False)\n",
    "print(output_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585ms/step\n",
      "   SampleID  Category\n",
      "0         1         4\n",
      "1         2         2\n",
      "2         3         4\n",
      "3         4         4\n",
      "4         5         3\n"
     ]
    }
   ],
   "source": [
    "rnn_path = r'models pickled\\rnn_model2_0.723.h5'\n",
    "# Load the model\n",
    "rnn_model2 = load_model(rnn_path)\n",
    "rnn_model2_predictions = rnn_model2.predict(test_pad)\n",
    "\n",
    "\n",
    "predicted_classes = rnn_model2_predictions.argmax(axis=1)\n",
    "\n",
    "sample_ids = np.arange(1, len(predicted_classes) + 1)\n",
    "    \n",
    "output_df = pd.DataFrame({\n",
    "    \"SampleID\": sample_ids,\n",
    "    \"Category\": predicted_classes\n",
    "})\n",
    "    \n",
    "output_df.to_csv('rnn2_test.csv', index=False)\n",
    "print(output_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000214E42C7380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000214E42C7380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 236ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000214E42C7380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000214E42C7380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "   SampleID  Category\n",
      "0         1         2\n",
      "1         2         2\n",
      "2         3         2\n",
      "3         4         4\n",
      "4         5         3\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "cnn_path = r'models pickled\\cnn_model_0.717.h5'\n",
    "cnn_model = load_model(cnn_path)\n",
    "cnn_model_predictions = cnn_model.predict(test_pad)\n",
    "\n",
    "predicted_classes = cnn_model_predictions.argmax(axis=1)\n",
    "\n",
    "sample_ids = np.arange(1, len(predicted_classes) + 1)\n",
    "    \n",
    "output_df = pd.DataFrame({\n",
    "    \"SampleID\": sample_ids,\n",
    "    \"Category\": predicted_classes\n",
    "})\n",
    "    \n",
    "output_df.to_csv('cnn_test.csv', index=False)\n",
    "print(output_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_len, embed_dim):\n",
    "    positional_encoding = np.zeros((seq_len, embed_dim))\n",
    "\n",
    "    for pos in tqdm(range(seq_len), desc=\"Generating Positional Encoding\"):\n",
    "        for i in range(embed_dim):\n",
    "            angle = pos / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "            if i % 2 == 0:\n",
    "                positional_encoding[pos, i] = np.sin(angle)  # Apply sine to even indices\n",
    "            else:\n",
    "                positional_encoding[pos, i] = np.cos(angle)  # Apply cosine to odd indices\n",
    "    return tf.cast(positional_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "transformer_path = r'models pickled\\\\transformer_model_0.71.h5'\n",
    "\n",
    "transformer_model = load_model(\n",
    "    transformer_path, \n",
    "    custom_objects={'get_positional_encoding': get_positional_encoding, \n",
    "                    'MultiHeadAttention': MultiHeadAttention}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
